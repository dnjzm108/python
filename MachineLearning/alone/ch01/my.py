# -*- coding: utf-8 -*-
"""혼자하는 머신러닝+딥러닝_ch01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lBcTGP_j16dcTO7K4kPHPSj54MYRBBQX
"""

import sklearn
import tensorflow
import matplotlib.pyplot as plt
from random import *

"""2개의 클래스 (class)
분류(classfication)
이진 분류(binary classfication)
"""

# 샘플 준비 도미 와 빙어
# 도미 샘플
bream_length = [round(random(),2)*10 + 20 + i  for i in range(1,36)]
bream_weight = [randint(50,100) + 150 + i * randint(10,15) for i in range(1,36)]

# 빙어 샘플
smelt_length = [round(random(),1) + 9 + i/2 for i in range(1,15)]
smelt_length.sort()
smelt_weight = [round(random(),1)* 10 + 9 + i for i in range(1,15)]
smelt_weight.sort()

# # scatter plot  산점도 
# plt.scatter(bream_length,bream_weight)
# plt.xlabel('length')
# plt.ylabel('weight')
# plt.show()

# 리스트 합치기
length = bream_length + smelt_length
weight = bream_weight + smelt_weight
fish_data = [[l,w] for l,w in zip(length,weight)]

# 정답 준비
fish_target = [1]*35 + [0]*14

# k-최근접 이웃
from sklearn.neighbors import KNeighborsClassifier

# 모델
kn = KNeighborsClassifier()

kn.fit(fish_data, fish_target)

kn.score(fish_data, fish_target)

kn.predict([[30,600]])

kn49 = KNeighborsClassifier(n_neighbors=49)
kn49.fit(fish_data,fish_target)
kn49.score(fish_data,fish_target)

# print(35/49) 확률

"""# 지도 학습 과 비지도 학습
k-최근접 이웃 => 지도학습  타겟 데이터가 를 가르치는 학습하는 알고리즘
비지도 학습 - 타겟 데이터 없이 입력 데이터 만으로 하는 알고리즘
강화 학습 - 어떤 행동을 수행 후 주변에 환경에서 결과를 피드백 받아 개선해나가는 알고리즘
"""

# 훈련 세트 와 테스트 세트

train_input = fish_data[:35]
train_target = fish_target[:35]

test_input = fish_data[35:]
test_target = fish_target[35:]

# 테스트 세트에서 평가하기

from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier()
kn = kn.fit(train_input,train_target)

kn.score(test_input,test_target)

"""샘플링 편향

샘플링은 골고루 섞어서 훈련세트 와 테스트 세트를 나눠 주어야 한다.
"""

import numpy as np

input_arr = np.array(fish_data)
target_arr = np.array(fish_target)

index = np.arange(49)
np.random.shuffle(index)

train_input = input_arr[index[:35]]
train_target = target_arr[index[:35]]

test_input = input_arr[index[35:]]
test_target = target_arr[index[35:]]

plt.scatter(train_input[:,0],train_input[:,1])
plt.scatter(test_input[:,0],test_input[:,1])
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

# 두번째 머신 러닝 프로그램

kn = kn.fit(train_input,train_target)
kn.score(test_input,test_target)
kn.predict(test_input)

"""## 데이터 전처리
넘파이로 데이터 준비
"""

# stack 세로로 쌓이면서 인자가 2개면 2차 배열 
fish_data = np.column_stack((length,weight))
# concatenate 뒤로 붙여주는 함수 배열을 합쳐줌
fish_target = np.concatenate((np.ones(35), np.zeros(14)))

# np.full((2,3),9) 2줄 3칸 배열을 9로 채워줌

"""사이킷런으로 데이터 나누기"""

from sklearn.model_selection import train_test_split

train_input,test_input,train_target,test_target = train_test_split(fish_data,fish_target,stratify=fish_target,random_state=42)

kn = KNeighborsClassifier()

kn.fit(train_input,train_target)
kn.score(test_input,test_target)

print(kn.predict([[25,150]]))

distances,indexes = kn.kneighbors([[25,150]])

plt.scatter(train_input[:,0],train_input[:,1])
plt.scatter(25,150,marker='^')
plt.scatter(train_input[indexes,0],train_input[indexes,1],marker='D')

plt.xlabel('length')
plt.ylabel('weight')
plt.show()

# 기준을 맞춰라

plt.scatter(train_input[:,0],train_input[:,1])
plt.scatter(25,150,marker='^')
plt.scatter(train_input[indexes,0],train_input[indexes,1],marker='D')

# x,y축 조절 스케일을 맞춰 주는 것
plt.xlim((0,1000))
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

# 표준 점수 로 바꾸기

# 평균 구하기  axis =1 경우 가로로 개별의 평균을 각각 구한다. 0은 세로
mean = np.mean(train_input,axis=0)
# 표준 편차 구하기
std = np.std(train_input,axis=0)
print(mean,std)

# 넘파이 배열끼리는 자동으로 브로드 캐스팅 하여 계산 해준다.
train_scaled = (train_input - mean) / std
print(train_scaled)

#  수상한 도미 다시 표시하기

new = ([25,150] - mean) / std

plt.scatter(train_scaled[:,0],train_scaled[:,1])
plt.scatter(new[0],new[1],marker='^')

plt.xlabel('lenght')
plt.ylabel('weight')
plt.show()

# 전처리 데이터 에서 모델 훈련

kn.fit(train_scaled,train_target)
test_scaled = (test_input - mean) / std
kn.score(test_scaled, test_target)

print(kn.predict([new]))

distances,indexes = kn.kneighbors([new])

plt.scatter(train_scaled[:,0],train_scaled[:,1])
plt.scatter(new[0],new[1],marker='^')
plt.scatter(train_scaled[indexes,0],train_scaled[indexes,1],marker='D')

plt.xlabel('length')
plt.ylabel('weight')
plt.show()