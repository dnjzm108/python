# -*- coding: utf-8 -*-
"""혼자하는 머신러닝+딥러닝_ch02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bidjgjKMM_d2HvEUQjwnNAe_FpoY-IND
"""

#회귀
import matplotlib.pyplot as plt
from random import *
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.neighbors import KNeighborsRegressor
# ~Classfier 분류 모델  ~Regressor 회귀모델

perch_length = [round(random(),2)*10 + 10 + i  for i in range(1,50)]
perch_length.sort()
perch_weight = [randint(50,100) * randint(1,10) + i for i in perch_length]
perch_weight.sort()

perch_length = np.array(perch_length)
perch_weight = np.array(perch_weight)

plt.scatter(perch_length,perch_weight)

plt.xlabel('lenght')
plt.ylabel('weight')
plt.show()

train_input,test_input,train_target,test_target = train_test_split(perch_length,perch_weight,random_state=42)

train_input = train_input.reshape(-1,1)
test_input = test_input.reshape(-1,1)
print('train_input : ',train_input)
print('test_input : ',test_input)

knr = KNeighborsRegressor()
knr.fit(train_input,train_target)

result = knr.score(test_input,test_target)
print('result : ',result)

from sklearn.metrics import mean_absolute_error

test_prediction = knr.predict(test_input)
mae = mean_absolute_error(test_target,test_prediction)
# 평균적으로 얼마나 차이가 나는지
print('mae : ',mae)

# 과대적합 과 과소 적합
train = knr.score(train_input,train_target)
print('train : ',train)
test = knr.score(test_input,test_target)
print('test : ',test)

#  이웃 개수가 적어지면 과대 적합   많아지면 과소적합
knr.n_neighbors = 3
knr.fit(train_input,train_target)

train = knr.score(train_input,train_target)
print('train : ',train)
test = knr.score(test_input,test_target)
print('test : ',test)

[try_test] = knr.predict([[100]])
print('try_test : ',try_test) 

plt.scatter(train_input,train_target)
plt.scatter(100,try_test,marker='^')
plt.scatter(test_input,test_target,marker='D')

plt.xlabel('length')
plt.ylabel('weight')
plt.show()

# 트레이닝 범위를 벋어나면 이상한 값을 가지게 된다. 
# 이유는 가장 가까운 값의 평균이기 때문에 트레이닝된 범위 에서만 사용 가능한 알고리즘이다.

"""# 선형 회귀

LinearRegression
"""

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
# 선형 회기 모델 훈련
lr.fit(train_input,train_target)

# 50cm 농어에 대한 예측
print(lr.predict([[50]]))
#      기울기  ,    y절편
# 농어무게 = 기울기(a) * 농어길이 + 절편(b)
print(lr.coef_,lr.intercept_)

# 학습한 직선 그리기

# 훈련 세트의 산점도를 그리기
plt.scatter(train_input,train_target)

# 15 에서 50까지 1차 방정식 그래프를 그립니다.
plt.plot([15,70],[15*lr.coef_ + lr.intercept_,70*lr.coef_+lr.intercept_])

# 50cm 농어 데이터
plt.scatter(50,585.3,marker='^')
plt.show()

print(lr.score(train_input,train_target))

print(lr.score(test_input,test_target))

# 다항 회귀

# 무게 = 기울기(제곱된 길이의 기울기) * 길이**2 + 기울기 * 길이 + 절편
# lr = LinearRegression()
train_poly = np.column_stack((train_input ** 2,train_input))
test_poly = np.column_stack((test_input ** 2 ,test_input))

lr.fit(train_poly,train_target)

print(lr.predict([[50**2 , 50]]))

print(lr.coef_,lr.intercept_)
# 무게 = 0.07 * 길이**2 - 6.69 * 길이 + 55.47

# 학습한 직선 그리기

# 구간별 직선을 그리기 위해 15에서 49까지  정수 배열을 만든다.
point = np.arange(15,70)

# 훈련 세트의 산점도를 그립니다.
plt.scatter(train_input,train_target)

# 15에서 49까지 2차 방정식 그래프를 그립니다.
plt.plot(point,  0.07 * point**2 + 6.6 * point + 55.4)

# 50cm 농어 데이터 
plt.scatter([50],[577.13],marker='^')
plt.show()

print(lr.score(train_poly,train_target))

print(lr.score(test_poly,test_target))

"""# 특성 공학 과 규제"""

# 다중 회귀  multiple regression
#  판다스로 데이터 준비 - 데이터 프레임 DataFrame

import pandas as pd

df = pd.read_csv('https://bit.ly/perch_csv')
perch_full = df.to_numpy()
# print(perch_full)

perch_full = np.array(perch_full)
# print(len(perch_full))

# 리스트 합치기 길이 와 높이 
perch_input = np.column_stack((perch_full[:,0] , perch_full[:,1]))
# 두께
perch_data = perch_full[:,2]

# print(perch_input)
# print(perch_data)

train_input,test_input,train_target,test_target = train_test_split(perch_input,perch_data,random_state=42)
print('train_input :',train_input)
print('test_input : ',test_input)
print('train_target : ',train_target)
print('test_target : ',test_target)

# 길이 높이 두께

# 다항 특성 만들기 
from sklearn.preprocessing import PolynomialFeatures

# degree =2
poly = PolynomialFeatures()
poly.fit([[2,3]])

# 1(bias), 2,3,2**2,2*3,3**2
print(poly.transform([[2,3]]))

# LinearRegression

poly = PolynomialFeatures(include_bias=False)

poly.fit(train_input)
train_poly = poly.transform(train_input)

print(train_poly.shape)

poly.get_feature_names()

test_poly = poly.transform(test_input)

from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(train_poly,train_target)

print(lr.score(train_poly,train_target))

print(lr.score(test_poly,test_target))

# 더 많은 특성 만들기

poly = PolynomialFeatures(degree=5,include_bias=False)

poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)

print(train_poly.shape)

lr.fit(train_poly,train_target)

print(lr.score(train_poly,train_target))

print(lr.score(test_poly,test_target))

# 규제 전에 표준화

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_poly)

train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)

print(train_scaled)
print(test_scaled)

# 릿지 회귀

from sklearn.linear_model import Ridge

ridge = Ridge()
ridge.fit(train_scaled,train_target)

print(ridge.score(train_scaled,train_target))

print(ridge.score(test_scaled,test_target))

# 적절한 규제 강도 찾기

alpha_list = [0.001,0.01,0.1,1,10,100]
train_score=[]
test_score=[]
for alpha in alpha_list:
  # 릿지 모델을 만듭니다.
  ridge = Ridge(alpha=alpha)
  # 릿지 모델을 훈련합니다.
  ridge.fit(train_scaled,train_target)
  # 훈련 점수와 테스트 점수를 저장합니다.
  train_score.append(ridge.score(train_scaled,train_target))
  test_score.append(ridge.score(test_scaled,test_target))

plt.plot(np.log10(alpha_list),train_score)
plt.plot(np.log10(alpha_list),test_score)
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()

ridge = Ridge(alpha =0.1)
ridge.fit(train_scaled,train_target)

print(ridge.score(train_scaled,train_target))

print(ridge.score(test_scaled,test_target))

# 라쏘 회귀

from sklearn.linear_model import Lasso

lasso = Lasso()
lasso.fit(train_scaled,train_target)

print(lasso.score(train_scaled,train_target))

print(lasso.score(test_scaled,test_target))

lass = Lasso(alpha=10)
lasso.fit(train_scaled,train_target)

print(lasso.score(train_scaled,train_target))

print(lasso.score(test_scaled,test_target))

print(np.sum(lasso.coef_ == 0))



